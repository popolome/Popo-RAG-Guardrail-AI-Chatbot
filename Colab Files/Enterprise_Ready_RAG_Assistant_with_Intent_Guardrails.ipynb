{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyO5ml4LqxtfB/XHkwa24jrj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/popolome/SmartDoc-A-Private-AI-Assistant-for-Your-Company-Files/blob/main/Enterprise_Ready_RAG_Assistant_with_Intent_Guardrails.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "* I used **Hybrid-Cloud Architecture** here by leveraging **Colab's L4 GPU** for vectorization, then use **Groq's API** for ultra-low latency inference.\n",
        "* Minimal GPU VRAM and RAM required, less than 8GB VRAM and 16GB RAM is enough, API handles the heavy-lifting.\n",
        "* I fine-tune the Guardrails prompt to handle various short-comings of the chatbot.\n",
        "* I also changed k=3 to k=10 in the end to make the bot 'smarter'.\n",
        "* Added memory and streaming to make the bot output more flowy and actually remembers past 3 conversations.\n",
        "* Tested conversing with the chatbot, and deployed to Streamlit."
      ],
      "metadata": {
        "id": "c_sqb6_cs4vs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install dependencies"
      ],
      "metadata": {
        "id": "pZjW-HdSxLb2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIzcHQV_rnLy"
      },
      "outputs": [],
      "source": [
        "%%writefile requirements.txt\n",
        "langchain\n",
        "langchain-classic\n",
        "langchain-groq\n",
        "langchain-community\n",
        "langchain_huggingface\n",
        "langchain_text_splitters\n",
        "langchain_chroma\n",
        "langchain-core\n",
        "chromadb\n",
        "pypdf\n",
        "sentence-transformers\n",
        "streamlit\n",
        "python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "FsKReOIkyE8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up Groq API Key"
      ],
      "metadata": {
        "id": "E33gRNU9ypFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# Used getpass to hide your input when you type you Groq Key\n",
        "os.environ[\"GROQ_API_KEY\"] = getpass(\"Enter your Groq API Key here: \")"
      ],
      "metadata": {
        "id": "UUiAHQsJyJV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize the Embedding Model (Hugging Face)"
      ],
      "metadata": {
        "id": "BpbqMPYF23VE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "# This embeds the text into numerical embeddings that the Language Model can\n",
        "# understand\n",
        "embeddings = HuggingFaceEmbeddings(model=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "print(\"Embedding model loaded successfully!\")"
      ],
      "metadata": {
        "id": "VixqMQ6Pz7nE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize the Language Model (Processor)"
      ],
      "metadata": {
        "id": "EeYsvWWB40CU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We use Groq LPU here for fast inference\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "# Setting Low Temperature(Less Random), High Temperature(More Random)\n",
        "llm = ChatGroq(\n",
        "    model_name=\"llama-3.1-8b-instant\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=30,\n",
        "    max_retries=2\n",
        ")\n",
        "\n",
        "print(\"LLM initialized with ChatGroq!\")"
      ],
      "metadata": {
        "id": "_mnmuIAx3oYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import or Drag the Apple_10K_Report.pdf or any PDFs to Colab folder\n",
        "* The folder is on the left."
      ],
      "metadata": {
        "id": "dCpSf_-rAnNM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and chunk the document"
      ],
      "metadata": {
        "id": "MYudLLB_AbCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Use PyPDFLoader to load our pdf report\n",
        "loader = PyPDFLoader(\"Apple_10K_Report.pdf\")\n",
        "docs = loader.load()\n",
        "\n",
        "# Use RecursiveCharacterTextSplitter to split each text into 1000 characters\n",
        "# and if more than 1000 characters in chunk 1, put them in chunk 2\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "chunks = text_splitter.split_documents(docs)\n",
        "\n",
        "print(f\"Document loaded. Total chunks created: {len(chunks)}\")"
      ],
      "metadata": {
        "id": "SUYa7Z0A--rH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Vector store"
      ],
      "metadata": {
        "id": "F4X24m10p1IR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "\n",
        "# Convert our chunks from earlier into vectors\n",
        "# Save the vector database as well\n",
        "vector_db = Chroma.from_documents(\n",
        "    documents=chunks,\n",
        "    embedding=embeddings,\n",
        "    persist_directory=\"./apple_chroma_db\"\n",
        ")\n",
        "\n",
        "print(\"Vector database has been created and saved!\")"
      ],
      "metadata": {
        "id": "3VdcXOxjoxnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the Guardrail Chatbot"
      ],
      "metadata": {
        "id": "bfAbiKu_sd9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_classic.memory import ConversationBufferWindowMemory\n",
        "from langchain_classic.chains import ConversationalRetrievalChain\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# This is the guardrail template to fallback if someone asks it a unanswerable\n",
        "# question, aka \"hallucination\"\n",
        "template = \"\"\"\n",
        "### ROLE\n",
        "You are Popo, a Senior Financial Analyst specializing in Apple Inc. Your tone is professional, objective, and precise.\n",
        "\n",
        "### INSTRUCTIONS\n",
        "1. **Scope Control**: Use ONLY the provided context and chat history. If the information isn't there, say: \"I'm sorry, I only have the ability to answer questions about the provided Apple 10-K report.\"\n",
        "2. **Precision**: Always specify the exact fiscal year (e.g., 'In fiscal year 2025...').\n",
        "3. **Social Guardrail**: If the user greets you or says 'thank you', respond warmly as Popo and offer to assist with further analysis of the 10-K.\n",
        "4. **Context Awareness**: Use the history to handle follow-up questions accurately.\n",
        "5. **Ethical Boundary**: Strictly refuse to give personal investment advice. If asked, politely explain that your expertise is limited to analyzing the facts within the Apple 10-K report and suggest the user consult a certified financial advisor.\n",
        "6. **Formatting**: Use bullet points for lists of risks or financial metrics to improve readability.\n",
        "7. Identity: Do not assume the user's name or identity. Address the user respectfully as \"User\" or simply dive into the analysis unless they explicitly introduce themselves.\n",
        "\n",
        "### CONTEXT\n",
        "{context}\n",
        "\n",
        "### CHAT HISTORY\n",
        "{chat_history}\n",
        "\n",
        "### USER QUERY\n",
        "{question}\n",
        "\n",
        "### POPO's ANALYSIS:\n",
        "\"\"\"\n",
        "\n",
        "qa_prompt = PromptTemplate(\n",
        "      template=template,\n",
        "      input_variables=['context', 'chat_history' 'question']\n",
        "    )\n",
        "\n",
        "# This is the memory for the chatbot, it remembers the past 3 conversations\n",
        "memory = ConversationBufferWindowMemory(\n",
        "    memory_key='chat_history',\n",
        "    k=3,\n",
        "    return_messages=True,\n",
        "    output_keys='answer'\n",
        ")\n",
        "\n",
        "# The ConversationalRetrieval chain, the Apple bot\n",
        "# It assigns the Groq Llama3 as the llm, searches vector db for top 10 results\n",
        "# And to follow my prompting rules instead of the default\n",
        "apple_bot = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    chain_type='stuff',\n",
        "    retriever=vector_db.as_retriever(search_kwargs={'k': 10}),\n",
        "    memory=memory,\n",
        "    combine_docs_chain_kwargs={'prompt': qa_prompt}\n",
        ")\n",
        "\n",
        "print(\"Popo is online and ready!\")"
      ],
      "metadata": {
        "id": "Hg-NC5gJqLVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the Guardrails"
      ],
      "metadata": {
        "id": "qIsn6_Fx1U2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import time\n",
        "\n",
        "print((\"--- Apple 10-K Financial Analyst (v1.0) ---\"))\n",
        "print(\"Hello! My name is Popo, how may I assist you today? Type 'exit' to quit.\" )\n",
        "\n",
        "while True:\n",
        "  user_query = input(\"\\nYou: \")\n",
        "\n",
        "  if user_query.lower() in ['exit', 'quit', 'q']:\n",
        "    print(\"Goodbye! Popo is signing off.\")\n",
        "    break\n",
        "\n",
        "  # Creates a temporary thinking process till it replies\n",
        "  print(\"Popo is thinking...\", end=\"\\r\")\n",
        "\n",
        "  try:\n",
        "    print(f\"Popo: \", end=\"\")\n",
        "\n",
        "    # This is to loop thru each chunk in the stream and look for answer\n",
        "    for chunk in apple_bot.stream({\"question\": user_query}):\n",
        "      if 'answer' in chunk:\n",
        "        answer_text = chunk['answer']\n",
        "\n",
        "        # This creates a typewriter style chatbot lower time.sleep = faster\n",
        "        for char in answer_text:\n",
        "          sys.stdout.write(char)\n",
        "          sys.stdout.flush()\n",
        "          time.sleep(0.01)\n",
        "\n",
        "    # This is just an empty line for each answer\n",
        "    print()\n",
        "\n",
        "  except Exception as e:\n",
        "    if \"429\" in str(e):\n",
        "        print(\"\\n[Rate Limit] Popo needs a 60-minute breather, please wait...\")\n",
        "        time.sleep(5)\n",
        "    else:\n",
        "      print(f\"\\nAn error has occurred: {e}\")\n",
        "\n",
        "  print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "Fcy4EOf6szoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This is for securing my vector database for deployment"
      ],
      "metadata": {
        "id": "CgXFK9xNk0en"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# This is for zipping my vector database for download\n",
        "shutil.make_archive('apple_chroma_db_export', 'zip', 'apple_chroma_db')\n",
        "print(\"Database zipped! Look for 'apple_chroma_db_export.zip in your file menu.'\")"
      ],
      "metadata": {
        "id": "STCeHAFu17AO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
